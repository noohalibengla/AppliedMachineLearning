{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3L9Rx73KOvR",
        "outputId": "c3d24899-a2db-4d90-ebe2-a6da9952b377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpha_vantage\n",
        "\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import pandas as pd\n",
        "\n",
        "api_key = 'QO84EVQ44QFRY1VY'\n",
        "\n",
        "ts = TimeSeries(key=api_key, output_format='pandas')\n",
        "data, meta = ts.get_daily(symbol='MSFT', outputsize='full')\n",
        "\n",
        "# Clean and sort\n",
        "data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "data = data.sort_index()\n",
        "print(data.tail())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os0cp2NCQdoW",
        "outputId": "724c30a9-4e5c-425a-bc9e-094f9a1c4966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpha_vantage\n",
            "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (3.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2025.4.26)\n",
            "Downloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: alpha_vantage\n",
            "Successfully installed alpha_vantage-3.0.0\n",
            "              Open    High       Low   Close      Volume\n",
            "date                                                    \n",
            "2025-05-02  431.74  439.44  429.9850  435.28  30757434.0\n",
            "2025-05-05  432.87  439.50  432.1100  436.17  20136053.0\n",
            "2025-05-06  432.20  437.73  431.1700  433.31  15104204.0\n",
            "2025-05-07  433.84  438.12  431.1103  433.35  23307241.0\n",
            "2025-05-08  437.93  443.67  435.6600  438.17  23491330.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va5pj2RuQo6e",
        "outputId": "4421ae14-fb85-4113-aa92-e337cbc75179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6419"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocess"
      ],
      "metadata": {
        "id": "tAWtdX2oFDAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Compute returns and moving averages\n",
        "data['Return'] = data['Close'].pct_change()\n",
        "data['MA5'] = data['Close'].rolling(window=5).mean()\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Step 2: Normalize features (you can also use sklearn StandardScaler)\n",
        "data[['Return', 'MA5', 'MA10', 'Volume']] = data[['Return', 'MA5', 'MA10', 'Volume']].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# Step 3: Create state windows\n",
        "window_size = 10\n",
        "features = ['Return', 'MA5', 'MA10', 'Volume']\n",
        "states = []\n",
        "for i in range(window_size, len(data)):\n",
        "    window = data[features].iloc[i-window_size:i].values.flatten()\n",
        "    states.append(window)\n",
        "\n",
        "states = np.array(states)  # shape: (samples, window_size * num_features)\n",
        "\n",
        "print(f\"State shape: {states.shape}\")  # Should be (samples, 10 × 4 = 40)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elp8wCN6Ulwi",
        "outputId": "b6de3173-91a9-4a91-952d-e620684500ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape: (6400, 40)\n",
            "6410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[data.index < '2025-04-01']\n",
        "test_data = data[data.index >= '2025-04-01']\n"
      ],
      "metadata": {
        "id": "gZNuJCi9GTGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep aligned dates for each state\n",
        "state_dates = data.index[10:]  # Each state ends at this date\n",
        "\n",
        "# Convert to NumPy array if needed\n",
        "state_dates = pd.to_datetime(data.index[10:])\n",
        "\n",
        "\n",
        "# Boolean masks\n",
        "train_mask = state_dates < '2025-04-01'\n",
        "test_mask = state_dates >= '2025-04-01'\n",
        "\n",
        "# Split states\n",
        "train_states = states[train_mask]\n",
        "test_states = states[test_mask]\n",
        "\n",
        "print(train_states.shape)\n",
        "print(test_states.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUhp4xmqZNYV",
        "outputId": "9619bc9f-a9f8-4431-c7bf-552dec42f71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6373, 40)\n",
            "(26, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QNetwork"
      ],
      "metadata": {
        "id": "lT92gsJME6GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, output_dim=3):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)  # No softmax — raw Q-values\n"
      ],
      "metadata": {
        "id": "vnlIWfKlZyQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReplayBuffer"
      ],
      "metadata": {
        "id": "UQXS3N-iE3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards, dtype=np.float32),\n",
        "            np.array(next_states),\n",
        "            np.array(dones, dtype=np.uint8),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "Xl9Tv4LXCFE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TradingEnv setup"
      ],
      "metadata": {
        "id": "1zEGxYw8EkFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv:\n",
        "    def __init__(self, states, prices, initial_balance=10000):\n",
        "        self.states = states\n",
        "        self.prices = prices  # usually Close prices\n",
        "        self.initial_balance = initial_balance\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.total_asset = self.initial_balance\n",
        "        self.done = False\n",
        "        return self.states[self.current_step]\n",
        "\n",
        "    def step(self, action):\n",
        "        # Actions: 0 = Hold, 1 = Buy, 2 = Sell\n",
        "        price = self.prices[self.current_step]\n",
        "\n",
        "        # Buy\n",
        "        if action == 1 and self.balance >= price:\n",
        "            self.shares_held += 1\n",
        "            self.balance -= price\n",
        "\n",
        "        # Sell\n",
        "        elif action == 2 and self.shares_held > 0:\n",
        "            self.shares_held -= 1\n",
        "            self.balance += price\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= len(self.states) - 1:\n",
        "            self.done = True\n",
        "\n",
        "        next_state = self.states[self.current_step]\n",
        "        next_price = self.prices[self.current_step]\n",
        "\n",
        "        new_total_asset = self.balance + self.shares_held * next_price\n",
        "        reward = new_total_asset - self.total_asset\n",
        "        self.total_asset = new_total_asset\n",
        "\n",
        "        return next_state, reward, self.done, self.balance, self.shares_held,price\n"
      ],
      "metadata": {
        "id": "lIEw52juCTuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `train_states` is your (N, 40) NumPy array\n",
        "# Assume `train_data` is your DataFrame with 'Close' prices\n",
        "# Use 'Close' as the price input for reward calculation\n",
        "env = TradingEnv(states=train_states, prices=train_data['Close'].values)\n",
        "\n",
        "state = env.reset()\n",
        "print(\"Initial total asset:\", env.total_asset)\n",
        "\n",
        "for step in range(10):  # test first 10 steps\n",
        "    action = np.random.choice([0, 1, 2])  # randomly pick action\n",
        "    next_state, reward, done,balance,shares_hold,price = env.step(action)\n",
        "\n",
        "    print(f\"Step {step + 1}\")\n",
        "    print(\"  Action:\", [\"Hold\", \"Buy\", \"Sell\"][action])\n",
        "    print(\"  Reward:\", reward)\n",
        "    print(\"  Total Asset:\", env.total_asset)\n",
        "    if done:\n",
        "        print(\"Episode ended early.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuAH0hS_F9JA",
        "outputId": "1f8e15bb-93a6-4c0c-a16d-aeb56dfb649c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial total asset: 10000\n",
            "Step 1\n",
            "  Action: Buy\n",
            "  Reward: -2.1900000000005093\n",
            "  Total Asset: 9997.81\n",
            "Step 2\n",
            "  Action: Sell\n",
            "  Reward: 0.0\n",
            "  Total Asset: 9997.81\n",
            "Step 3\n",
            "  Action: Hold\n",
            "  Reward: 0.0\n",
            "  Total Asset: 9997.81\n",
            "Step 4\n",
            "  Action: Hold\n",
            "  Reward: 0.0\n",
            "  Total Asset: 9997.81\n",
            "Step 5\n",
            "  Action: Sell\n",
            "  Reward: 0.0\n",
            "  Total Asset: 9997.81\n",
            "Step 6\n",
            "  Action: Hold\n",
            "  Reward: 0.0\n",
            "  Total Asset: 9997.81\n",
            "Step 7\n",
            "  Action: Buy\n",
            "  Reward: -0.18999999999869033\n",
            "  Total Asset: 9997.62\n",
            "Step 8\n",
            "  Action: Buy\n",
            "  Reward: 4.139999999997599\n",
            "  Total Asset: 10001.759999999998\n",
            "Step 9\n",
            "  Action: Hold\n",
            "  Reward: -1.139999999999418\n",
            "  Total Asset: 10000.619999999999\n",
            "Step 10\n",
            "  Action: Buy\n",
            "  Reward: -2.790000000000873\n",
            "  Total Asset: 9997.829999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize Q-network and target network\n",
        "policy_net = QNetwork().to(device)\n",
        "target_net = QNetwork().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "\n",
        "# Replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "gDNCpdWuIJO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 500\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 0.995\n",
        "target_update_freq = 5  # Episodes after which to sync target network\n"
      ],
      "metadata": {
        "id": "cUPFivU4IozP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to select action (ε-greedy)\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice([0, 1, 2])  # hold, buy, sell\n",
        "    else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = policy_net(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "# Extract corresponding Close prices for rewards\n",
        "train_prices = data['Close'].values[10:][train_mask]\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    env = TradingEnv(train_states, train_prices)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(state, epsilon)\n",
        "        next_state, reward, done, balance, shares_hold, price = env.step(action)\n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            # Sample batch\n",
        "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
        "\n",
        "            states_b = torch.FloatTensor(states_b).to(device)\n",
        "            actions_b = torch.LongTensor(actions_b).unsqueeze(1).to(device)\n",
        "            rewards_b = torch.FloatTensor(rewards_b).to(device)\n",
        "            next_states_b = torch.FloatTensor(next_states_b).to(device)\n",
        "            dones_b = torch.BoolTensor(dones_b).to(device)\n",
        "\n",
        "            # Q(s, a)\n",
        "            q_values = policy_net(states_b).gather(1, actions_b).squeeze()\n",
        "\n",
        "            # target = r + γ max_a Q_target(s', a)\n",
        "            with torch.no_grad():\n",
        "                next_q_values = target_net(next_states_b).max(1)[0]\n",
        "                targets = rewards_b + gamma * next_q_values * (~dones_b)\n",
        "\n",
        "            # Compute loss and update\n",
        "            loss = loss_fn(q_values, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Sync target network\n",
        "    if (episode + 1) % target_update_freq == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode + 1}/{num_episodes} — Total Reward: {total_reward:.2f} — Epsilon: {epsilon:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXKMyueKI6cs",
        "outputId": "76907d5f-ffae-4f4f-bac8-d7e621878a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/500 — Total Reward: 49106.07 — Epsilon: 1.000\n",
            "Episode 2/500 — Total Reward: 7854.45 — Epsilon: 0.995\n",
            "Episode 3/500 — Total Reward: 7704.83 — Epsilon: 0.990\n",
            "Episode 4/500 — Total Reward: 26839.34 — Epsilon: 0.985\n",
            "Episode 5/500 — Total Reward: 17367.45 — Epsilon: 0.980\n",
            "Episode 6/500 — Total Reward: 18059.47 — Epsilon: 0.975\n",
            "Episode 7/500 — Total Reward: 4408.85 — Epsilon: 0.970\n",
            "Episode 8/500 — Total Reward: 7658.21 — Epsilon: 0.966\n",
            "Episode 9/500 — Total Reward: 19565.26 — Epsilon: 0.961\n",
            "Episode 10/500 — Total Reward: 7512.84 — Epsilon: 0.956\n",
            "Episode 11/500 — Total Reward: 20970.99 — Epsilon: 0.951\n",
            "Episode 12/500 — Total Reward: 22098.58 — Epsilon: 0.946\n",
            "Episode 13/500 — Total Reward: 34464.13 — Epsilon: 0.942\n",
            "Episode 14/500 — Total Reward: 12567.65 — Epsilon: 0.937\n",
            "Episode 15/500 — Total Reward: 4478.64 — Epsilon: 0.932\n",
            "Episode 16/500 — Total Reward: 4260.07 — Epsilon: 0.928\n",
            "Episode 17/500 — Total Reward: 4977.09 — Epsilon: 0.923\n",
            "Episode 18/500 — Total Reward: 17717.92 — Epsilon: 0.918\n",
            "Episode 19/500 — Total Reward: 9981.84 — Epsilon: 0.914\n",
            "Episode 20/500 — Total Reward: 1138.24 — Epsilon: 0.909\n",
            "Episode 21/500 — Total Reward: 15626.01 — Epsilon: 0.905\n",
            "Episode 22/500 — Total Reward: 12557.89 — Epsilon: 0.900\n",
            "Episode 23/500 — Total Reward: 5755.17 — Epsilon: 0.896\n",
            "Episode 24/500 — Total Reward: 26177.09 — Epsilon: 0.891\n",
            "Episode 25/500 — Total Reward: 4400.93 — Epsilon: 0.887\n",
            "Episode 26/500 — Total Reward: 1970.99 — Epsilon: 0.882\n",
            "Episode 27/500 — Total Reward: 7758.67 — Epsilon: 0.878\n",
            "Episode 28/500 — Total Reward: 19695.76 — Epsilon: 0.873\n",
            "Episode 29/500 — Total Reward: 10285.69 — Epsilon: 0.869\n",
            "Episode 30/500 — Total Reward: 14649.13 — Epsilon: 0.865\n",
            "Episode 31/500 — Total Reward: 4057.01 — Epsilon: 0.860\n",
            "Episode 32/500 — Total Reward: 1426.74 — Epsilon: 0.856\n",
            "Episode 33/500 — Total Reward: 4130.31 — Epsilon: 0.852\n",
            "Episode 34/500 — Total Reward: 4513.04 — Epsilon: 0.848\n",
            "Episode 35/500 — Total Reward: 2792.14 — Epsilon: 0.843\n",
            "Episode 36/500 — Total Reward: 8176.65 — Epsilon: 0.839\n",
            "Episode 37/500 — Total Reward: 2501.12 — Epsilon: 0.835\n",
            "Episode 38/500 — Total Reward: 5890.17 — Epsilon: 0.831\n",
            "Episode 39/500 — Total Reward: 2724.98 — Epsilon: 0.827\n",
            "Episode 40/500 — Total Reward: 2765.10 — Epsilon: 0.822\n",
            "Episode 41/500 — Total Reward: 3304.82 — Epsilon: 0.818\n",
            "Episode 42/500 — Total Reward: 5495.14 — Epsilon: 0.814\n",
            "Episode 43/500 — Total Reward: 7005.90 — Epsilon: 0.810\n",
            "Episode 44/500 — Total Reward: 11576.41 — Epsilon: 0.806\n",
            "Episode 45/500 — Total Reward: 4781.25 — Epsilon: 0.802\n",
            "Episode 46/500 — Total Reward: 5490.76 — Epsilon: 0.798\n",
            "Episode 47/500 — Total Reward: 6677.33 — Epsilon: 0.794\n",
            "Episode 48/500 — Total Reward: 8531.28 — Epsilon: 0.790\n",
            "Episode 49/500 — Total Reward: 30192.83 — Epsilon: 0.786\n",
            "Episode 50/500 — Total Reward: 19173.98 — Epsilon: 0.782\n",
            "Episode 51/500 — Total Reward: 39445.40 — Epsilon: 0.778\n",
            "Episode 52/500 — Total Reward: 11468.79 — Epsilon: 0.774\n",
            "Episode 53/500 — Total Reward: 7315.74 — Epsilon: 0.771\n",
            "Episode 54/500 — Total Reward: 7659.45 — Epsilon: 0.767\n",
            "Episode 55/500 — Total Reward: 5604.94 — Epsilon: 0.763\n",
            "Episode 56/500 — Total Reward: 5230.28 — Epsilon: 0.759\n",
            "Episode 57/500 — Total Reward: 15897.53 — Epsilon: 0.755\n",
            "Episode 58/500 — Total Reward: 18037.50 — Epsilon: 0.751\n",
            "Episode 59/500 — Total Reward: 17435.91 — Epsilon: 0.748\n",
            "Episode 60/500 — Total Reward: 3753.51 — Epsilon: 0.744\n",
            "Episode 61/500 — Total Reward: 4246.31 — Epsilon: 0.740\n",
            "Episode 62/500 — Total Reward: 14936.40 — Epsilon: 0.737\n",
            "Episode 63/500 — Total Reward: 2481.19 — Epsilon: 0.733\n",
            "Episode 64/500 — Total Reward: 16014.53 — Epsilon: 0.729\n",
            "Episode 65/500 — Total Reward: 20327.87 — Epsilon: 0.726\n",
            "Episode 66/500 — Total Reward: 2449.79 — Epsilon: 0.722\n",
            "Episode 67/500 — Total Reward: 5259.55 — Epsilon: 0.718\n",
            "Episode 68/500 — Total Reward: 40907.44 — Epsilon: 0.715\n",
            "Episode 69/500 — Total Reward: 8366.95 — Epsilon: 0.711\n",
            "Episode 70/500 — Total Reward: 708.63 — Epsilon: 0.708\n",
            "Episode 71/500 — Total Reward: 60965.57 — Epsilon: 0.704\n",
            "Episode 72/500 — Total Reward: 24160.35 — Epsilon: 0.701\n",
            "Episode 73/500 — Total Reward: 9477.57 — Epsilon: 0.697\n",
            "Episode 74/500 — Total Reward: 28187.93 — Epsilon: 0.694\n",
            "Episode 75/500 — Total Reward: 18295.29 — Epsilon: 0.690\n",
            "Episode 76/500 — Total Reward: 17761.54 — Epsilon: 0.687\n",
            "Episode 77/500 — Total Reward: 74603.15 — Epsilon: 0.683\n",
            "Episode 78/500 — Total Reward: 13700.54 — Epsilon: 0.680\n",
            "Episode 79/500 — Total Reward: 12185.86 — Epsilon: 0.676\n",
            "Episode 80/500 — Total Reward: 17203.20 — Epsilon: 0.673\n",
            "Episode 81/500 — Total Reward: 18426.41 — Epsilon: 0.670\n",
            "Episode 82/500 — Total Reward: 61893.46 — Epsilon: 0.666\n",
            "Episode 83/500 — Total Reward: 7457.93 — Epsilon: 0.663\n",
            "Episode 84/500 — Total Reward: 87606.98 — Epsilon: 0.660\n",
            "Episode 85/500 — Total Reward: 61469.69 — Epsilon: 0.656\n",
            "Episode 86/500 — Total Reward: 14852.28 — Epsilon: 0.653\n",
            "Episode 87/500 — Total Reward: 34278.90 — Epsilon: 0.650\n",
            "Episode 88/500 — Total Reward: 42696.88 — Epsilon: 0.647\n",
            "Episode 89/500 — Total Reward: 24183.38 — Epsilon: 0.643\n",
            "Episode 90/500 — Total Reward: 30916.30 — Epsilon: 0.640\n",
            "Episode 91/500 — Total Reward: 4904.16 — Epsilon: 0.637\n",
            "Episode 92/500 — Total Reward: 34993.77 — Epsilon: 0.634\n",
            "Episode 93/500 — Total Reward: 22889.92 — Epsilon: 0.631\n",
            "Episode 94/500 — Total Reward: 17555.66 — Epsilon: 0.627\n",
            "Episode 95/500 — Total Reward: 7267.71 — Epsilon: 0.624\n",
            "Episode 96/500 — Total Reward: 8607.16 — Epsilon: 0.621\n",
            "Episode 97/500 — Total Reward: 25422.15 — Epsilon: 0.618\n",
            "Episode 98/500 — Total Reward: 7182.42 — Epsilon: 0.615\n",
            "Episode 99/500 — Total Reward: 17658.48 — Epsilon: 0.612\n",
            "Episode 100/500 — Total Reward: 53636.53 — Epsilon: 0.609\n",
            "Episode 101/500 — Total Reward: 8089.95 — Epsilon: 0.606\n",
            "Episode 102/500 — Total Reward: 2686.23 — Epsilon: 0.603\n",
            "Episode 103/500 — Total Reward: 21216.41 — Epsilon: 0.600\n",
            "Episode 104/500 — Total Reward: 16339.86 — Epsilon: 0.597\n",
            "Episode 105/500 — Total Reward: 6454.46 — Epsilon: 0.594\n",
            "Episode 106/500 — Total Reward: 24707.12 — Epsilon: 0.591\n",
            "Episode 107/500 — Total Reward: 3114.90 — Epsilon: 0.588\n",
            "Episode 108/500 — Total Reward: 3006.32 — Epsilon: 0.585\n",
            "Episode 109/500 — Total Reward: 26559.24 — Epsilon: 0.582\n",
            "Episode 110/500 — Total Reward: 2147.86 — Epsilon: 0.579\n",
            "Episode 111/500 — Total Reward: 45999.91 — Epsilon: 0.576\n",
            "Episode 112/500 — Total Reward: -1377.89 — Epsilon: 0.573\n",
            "Episode 113/500 — Total Reward: 24335.86 — Epsilon: 0.570\n",
            "Episode 114/500 — Total Reward: 26241.24 — Epsilon: 0.568\n",
            "Episode 115/500 — Total Reward: 67483.14 — Epsilon: 0.565\n",
            "Episode 116/500 — Total Reward: 1807.30 — Epsilon: 0.562\n",
            "Episode 117/500 — Total Reward: 72688.84 — Epsilon: 0.559\n",
            "Episode 118/500 — Total Reward: 31683.54 — Epsilon: 0.556\n",
            "Episode 119/500 — Total Reward: 1002.37 — Epsilon: 0.554\n",
            "Episode 120/500 — Total Reward: 54494.99 — Epsilon: 0.551\n",
            "Episode 121/500 — Total Reward: 50762.16 — Epsilon: 0.548\n",
            "Episode 122/500 — Total Reward: 46451.63 — Epsilon: 0.545\n",
            "Episode 123/500 — Total Reward: 24408.58 — Epsilon: 0.543\n",
            "Episode 124/500 — Total Reward: 27049.06 — Epsilon: 0.540\n",
            "Episode 125/500 — Total Reward: 24714.02 — Epsilon: 0.537\n",
            "Episode 126/500 — Total Reward: 56283.56 — Epsilon: 0.534\n",
            "Episode 127/500 — Total Reward: 9022.10 — Epsilon: 0.532\n",
            "Episode 128/500 — Total Reward: 11239.53 — Epsilon: 0.529\n",
            "Episode 129/500 — Total Reward: 62874.95 — Epsilon: 0.526\n",
            "Episode 130/500 — Total Reward: 3090.92 — Epsilon: 0.524\n",
            "Episode 131/500 — Total Reward: 8206.41 — Epsilon: 0.521\n",
            "Episode 132/500 — Total Reward: 32444.37 — Epsilon: 0.519\n",
            "Episode 133/500 — Total Reward: 31535.30 — Epsilon: 0.516\n",
            "Episode 134/500 — Total Reward: 70386.74 — Epsilon: 0.513\n",
            "Episode 135/500 — Total Reward: 54080.63 — Epsilon: 0.511\n",
            "Episode 136/500 — Total Reward: 87671.14 — Epsilon: 0.508\n",
            "Episode 137/500 — Total Reward: 54594.73 — Epsilon: 0.506\n",
            "Episode 138/500 — Total Reward: 20357.32 — Epsilon: 0.503\n",
            "Episode 139/500 — Total Reward: 2518.62 — Epsilon: 0.501\n",
            "Episode 140/500 — Total Reward: 33654.02 — Epsilon: 0.498\n",
            "Episode 141/500 — Total Reward: 8284.98 — Epsilon: 0.496\n",
            "Episode 142/500 — Total Reward: 9019.96 — Epsilon: 0.493\n",
            "Episode 143/500 — Total Reward: 23597.69 — Epsilon: 0.491\n",
            "Episode 144/500 — Total Reward: 11200.41 — Epsilon: 0.488\n",
            "Episode 145/500 — Total Reward: 9071.08 — Epsilon: 0.486\n",
            "Episode 146/500 — Total Reward: 3061.12 — Epsilon: 0.483\n",
            "Episode 147/500 — Total Reward: 70098.09 — Epsilon: 0.481\n",
            "Episode 148/500 — Total Reward: 8833.07 — Epsilon: 0.479\n",
            "Episode 149/500 — Total Reward: 11329.94 — Epsilon: 0.476\n",
            "Episode 150/500 — Total Reward: 20379.55 — Epsilon: 0.474\n",
            "Episode 151/500 — Total Reward: 1029.67 — Epsilon: 0.471\n",
            "Episode 152/500 — Total Reward: 8611.97 — Epsilon: 0.469\n",
            "Episode 153/500 — Total Reward: 15536.53 — Epsilon: 0.467\n",
            "Episode 154/500 — Total Reward: 1004.14 — Epsilon: 0.464\n",
            "Episode 155/500 — Total Reward: 33869.81 — Epsilon: 0.462\n",
            "Episode 156/500 — Total Reward: 13017.56 — Epsilon: 0.460\n",
            "Episode 157/500 — Total Reward: 6638.29 — Epsilon: 0.458\n",
            "Episode 158/500 — Total Reward: 38101.36 — Epsilon: 0.455\n",
            "Episode 159/500 — Total Reward: 31863.46 — Epsilon: 0.453\n",
            "Episode 160/500 — Total Reward: 40778.19 — Epsilon: 0.451\n",
            "Episode 161/500 — Total Reward: 7446.39 — Epsilon: 0.448\n",
            "Episode 162/500 — Total Reward: 5101.55 — Epsilon: 0.446\n",
            "Episode 163/500 — Total Reward: 27428.80 — Epsilon: 0.444\n",
            "Episode 164/500 — Total Reward: 11331.39 — Epsilon: 0.442\n",
            "Episode 165/500 — Total Reward: 17777.44 — Epsilon: 0.440\n",
            "Episode 166/500 — Total Reward: 21495.56 — Epsilon: 0.437\n",
            "Episode 167/500 — Total Reward: 80293.04 — Epsilon: 0.435\n",
            "Episode 168/500 — Total Reward: 60944.45 — Epsilon: 0.433\n",
            "Episode 169/500 — Total Reward: 44095.32 — Epsilon: 0.431\n",
            "Episode 170/500 — Total Reward: 29808.74 — Epsilon: 0.429\n",
            "Episode 171/500 — Total Reward: 4250.02 — Epsilon: 0.427\n",
            "Episode 172/500 — Total Reward: 30284.22 — Epsilon: 0.424\n",
            "Episode 173/500 — Total Reward: 52445.36 — Epsilon: 0.422\n",
            "Episode 174/500 — Total Reward: 28839.83 — Epsilon: 0.420\n",
            "Episode 175/500 — Total Reward: 3979.51 — Epsilon: 0.418\n",
            "Episode 176/500 — Total Reward: 47804.22 — Epsilon: 0.416\n",
            "Episode 177/500 — Total Reward: 43343.09 — Epsilon: 0.414\n",
            "Episode 178/500 — Total Reward: 25737.96 — Epsilon: 0.412\n",
            "Episode 179/500 — Total Reward: 32561.79 — Epsilon: 0.410\n",
            "Episode 180/500 — Total Reward: 24566.33 — Epsilon: 0.408\n",
            "Episode 181/500 — Total Reward: 6990.84 — Epsilon: 0.406\n",
            "Episode 182/500 — Total Reward: 15891.25 — Epsilon: 0.404\n",
            "Episode 183/500 — Total Reward: 17230.43 — Epsilon: 0.402\n",
            "Episode 184/500 — Total Reward: 31215.59 — Epsilon: 0.400\n",
            "Episode 185/500 — Total Reward: 58953.44 — Epsilon: 0.398\n",
            "Episode 186/500 — Total Reward: 62433.63 — Epsilon: 0.396\n",
            "Episode 187/500 — Total Reward: 62691.10 — Epsilon: 0.394\n",
            "Episode 188/500 — Total Reward: 18444.54 — Epsilon: 0.392\n",
            "Episode 189/500 — Total Reward: 23113.16 — Epsilon: 0.390\n",
            "Episode 190/500 — Total Reward: 9147.29 — Epsilon: 0.388\n",
            "Episode 191/500 — Total Reward: 106284.42 — Epsilon: 0.386\n",
            "Episode 192/500 — Total Reward: 66949.72 — Epsilon: 0.384\n",
            "Episode 193/500 — Total Reward: 28478.76 — Epsilon: 0.382\n",
            "Episode 194/500 — Total Reward: 23318.66 — Epsilon: 0.380\n",
            "Episode 195/500 — Total Reward: 20852.10 — Epsilon: 0.378\n",
            "Episode 196/500 — Total Reward: 17417.18 — Epsilon: 0.376\n",
            "Episode 197/500 — Total Reward: 27381.47 — Epsilon: 0.374\n",
            "Episode 198/500 — Total Reward: 50777.84 — Epsilon: 0.373\n",
            "Episode 199/500 — Total Reward: 69021.13 — Epsilon: 0.371\n",
            "Episode 200/500 — Total Reward: 48183.61 — Epsilon: 0.369\n",
            "Episode 201/500 — Total Reward: 28235.60 — Epsilon: 0.367\n",
            "Episode 202/500 — Total Reward: 25909.59 — Epsilon: 0.365\n",
            "Episode 203/500 — Total Reward: 36991.99 — Epsilon: 0.363\n",
            "Episode 204/500 — Total Reward: 35713.15 — Epsilon: 0.361\n",
            "Episode 205/500 — Total Reward: 25212.01 — Epsilon: 0.360\n",
            "Episode 206/500 — Total Reward: 14600.27 — Epsilon: 0.358\n",
            "Episode 207/500 — Total Reward: 59856.60 — Epsilon: 0.356\n",
            "Episode 208/500 — Total Reward: 14941.03 — Epsilon: 0.354\n",
            "Episode 209/500 — Total Reward: 65568.08 — Epsilon: 0.353\n",
            "Episode 210/500 — Total Reward: 48752.08 — Epsilon: 0.351\n",
            "Episode 211/500 — Total Reward: 83690.88 — Epsilon: 0.349\n",
            "Episode 212/500 — Total Reward: 35511.63 — Epsilon: 0.347\n",
            "Episode 213/500 — Total Reward: 27474.46 — Epsilon: 0.346\n",
            "Episode 214/500 — Total Reward: 11431.09 — Epsilon: 0.344\n",
            "Episode 215/500 — Total Reward: 6862.71 — Epsilon: 0.342\n",
            "Episode 216/500 — Total Reward: 3181.66 — Epsilon: 0.340\n",
            "Episode 217/500 — Total Reward: 5738.51 — Epsilon: 0.339\n",
            "Episode 218/500 — Total Reward: 3599.09 — Epsilon: 0.337\n",
            "Episode 219/500 — Total Reward: 38380.70 — Epsilon: 0.335\n",
            "Episode 220/500 — Total Reward: 28068.84 — Epsilon: 0.334\n",
            "Episode 221/500 — Total Reward: 24234.74 — Epsilon: 0.332\n",
            "Episode 222/500 — Total Reward: 15508.66 — Epsilon: 0.330\n",
            "Episode 223/500 — Total Reward: 15063.57 — Epsilon: 0.329\n",
            "Episode 224/500 — Total Reward: 33661.40 — Epsilon: 0.327\n",
            "Episode 225/500 — Total Reward: 102845.05 — Epsilon: 0.325\n",
            "Episode 226/500 — Total Reward: 55214.93 — Epsilon: 0.324\n",
            "Episode 227/500 — Total Reward: -2731.88 — Epsilon: 0.322\n",
            "Episode 228/500 — Total Reward: 21758.46 — Epsilon: 0.321\n",
            "Episode 229/500 — Total Reward: 24896.09 — Epsilon: 0.319\n",
            "Episode 230/500 — Total Reward: 11410.40 — Epsilon: 0.317\n",
            "Episode 231/500 — Total Reward: 303.93 — Epsilon: 0.316\n",
            "Episode 232/500 — Total Reward: 12459.65 — Epsilon: 0.314\n",
            "Episode 233/500 — Total Reward: 9130.85 — Epsilon: 0.313\n",
            "Episode 234/500 — Total Reward: 17112.35 — Epsilon: 0.311\n",
            "Episode 235/500 — Total Reward: 32412.33 — Epsilon: 0.309\n",
            "Episode 236/500 — Total Reward: 19254.44 — Epsilon: 0.308\n",
            "Episode 237/500 — Total Reward: 1493.04 — Epsilon: 0.306\n",
            "Episode 238/500 — Total Reward: 1473.29 — Epsilon: 0.305\n",
            "Episode 239/500 — Total Reward: 25469.19 — Epsilon: 0.303\n",
            "Episode 240/500 — Total Reward: 12225.89 — Epsilon: 0.302\n",
            "Episode 241/500 — Total Reward: 1400.54 — Epsilon: 0.300\n",
            "Episode 242/500 — Total Reward: -2098.22 — Epsilon: 0.299\n",
            "Episode 243/500 — Total Reward: 2712.94 — Epsilon: 0.297\n",
            "Episode 244/500 — Total Reward: 5973.63 — Epsilon: 0.296\n",
            "Episode 245/500 — Total Reward: 6603.21 — Epsilon: 0.294\n",
            "Episode 246/500 — Total Reward: 24450.07 — Epsilon: 0.293\n",
            "Episode 247/500 — Total Reward: 6007.80 — Epsilon: 0.291\n",
            "Episode 248/500 — Total Reward: -1249.93 — Epsilon: 0.290\n",
            "Episode 249/500 — Total Reward: 17690.12 — Epsilon: 0.288\n",
            "Episode 250/500 — Total Reward: 17480.46 — Epsilon: 0.287\n",
            "Episode 251/500 — Total Reward: 5284.11 — Epsilon: 0.286\n",
            "Episode 252/500 — Total Reward: 3739.37 — Epsilon: 0.284\n",
            "Episode 253/500 — Total Reward: 9995.85 — Epsilon: 0.283\n",
            "Episode 254/500 — Total Reward: 6298.83 — Epsilon: 0.281\n",
            "Episode 255/500 — Total Reward: 5181.00 — Epsilon: 0.280\n",
            "Episode 256/500 — Total Reward: 10665.98 — Epsilon: 0.279\n",
            "Episode 257/500 — Total Reward: 46856.55 — Epsilon: 0.277\n",
            "Episode 258/500 — Total Reward: 26127.19 — Epsilon: 0.276\n",
            "Episode 259/500 — Total Reward: 16569.33 — Epsilon: 0.274\n",
            "Episode 260/500 — Total Reward: 3874.00 — Epsilon: 0.273\n",
            "Episode 261/500 — Total Reward: 1425.05 — Epsilon: 0.272\n",
            "Episode 262/500 — Total Reward: 4644.87 — Epsilon: 0.270\n",
            "Episode 263/500 — Total Reward: 29029.45 — Epsilon: 0.269\n",
            "Episode 264/500 — Total Reward: 14551.21 — Epsilon: 0.268\n",
            "Episode 265/500 — Total Reward: 5138.72 — Epsilon: 0.266\n",
            "Episode 266/500 — Total Reward: 2248.96 — Epsilon: 0.265\n",
            "Episode 267/500 — Total Reward: 29102.06 — Epsilon: 0.264\n",
            "Episode 268/500 — Total Reward: 26553.45 — Epsilon: 0.262\n",
            "Episode 269/500 — Total Reward: 23437.91 — Epsilon: 0.261\n",
            "Episode 270/500 — Total Reward: 30637.87 — Epsilon: 0.260\n",
            "Episode 271/500 — Total Reward: 48123.18 — Epsilon: 0.258\n",
            "Episode 272/500 — Total Reward: 13484.48 — Epsilon: 0.257\n",
            "Episode 273/500 — Total Reward: 11070.79 — Epsilon: 0.256\n",
            "Episode 274/500 — Total Reward: 11088.89 — Epsilon: 0.255\n",
            "Episode 275/500 — Total Reward: 5752.27 — Epsilon: 0.253\n",
            "Episode 276/500 — Total Reward: 409.50 — Epsilon: 0.252\n",
            "Episode 277/500 — Total Reward: 57820.47 — Epsilon: 0.251\n",
            "Episode 278/500 — Total Reward: 3656.31 — Epsilon: 0.249\n",
            "Episode 279/500 — Total Reward: 25807.21 — Epsilon: 0.248\n",
            "Episode 280/500 — Total Reward: 19005.34 — Epsilon: 0.247\n",
            "Episode 281/500 — Total Reward: 20248.64 — Epsilon: 0.246\n",
            "Episode 282/500 — Total Reward: 29227.80 — Epsilon: 0.245\n",
            "Episode 283/500 — Total Reward: 1782.58 — Epsilon: 0.243\n",
            "Episode 284/500 — Total Reward: 18134.48 — Epsilon: 0.242\n",
            "Episode 285/500 — Total Reward: 8341.59 — Epsilon: 0.241\n",
            "Episode 286/500 — Total Reward: 30264.04 — Epsilon: 0.240\n",
            "Episode 287/500 — Total Reward: 12118.52 — Epsilon: 0.238\n",
            "Episode 288/500 — Total Reward: 25332.32 — Epsilon: 0.237\n",
            "Episode 289/500 — Total Reward: 9008.97 — Epsilon: 0.236\n",
            "Episode 290/500 — Total Reward: 7874.43 — Epsilon: 0.235\n",
            "Episode 291/500 — Total Reward: 10470.49 — Epsilon: 0.234\n",
            "Episode 292/500 — Total Reward: 6971.35 — Epsilon: 0.233\n",
            "Episode 293/500 — Total Reward: 4553.73 — Epsilon: 0.231\n",
            "Episode 294/500 — Total Reward: 18402.78 — Epsilon: 0.230\n",
            "Episode 295/500 — Total Reward: 7912.69 — Epsilon: 0.229\n",
            "Episode 296/500 — Total Reward: 17362.33 — Epsilon: 0.228\n",
            "Episode 297/500 — Total Reward: 20975.65 — Epsilon: 0.227\n",
            "Episode 298/500 — Total Reward: 24475.25 — Epsilon: 0.226\n",
            "Episode 299/500 — Total Reward: 22968.02 — Epsilon: 0.225\n",
            "Episode 300/500 — Total Reward: 9675.28 — Epsilon: 0.223\n",
            "Episode 301/500 — Total Reward: 2493.44 — Epsilon: 0.222\n",
            "Episode 302/500 — Total Reward: 7801.02 — Epsilon: 0.221\n",
            "Episode 303/500 — Total Reward: 6060.90 — Epsilon: 0.220\n",
            "Episode 304/500 — Total Reward: 5950.38 — Epsilon: 0.219\n",
            "Episode 305/500 — Total Reward: -3993.62 — Epsilon: 0.218\n",
            "Episode 306/500 — Total Reward: -399.52 — Epsilon: 0.217\n",
            "Episode 307/500 — Total Reward: 13516.14 — Epsilon: 0.216\n",
            "Episode 308/500 — Total Reward: 4270.38 — Epsilon: 0.215\n",
            "Episode 309/500 — Total Reward: 8432.01 — Epsilon: 0.214\n",
            "Episode 310/500 — Total Reward: 23794.98 — Epsilon: 0.212\n",
            "Episode 311/500 — Total Reward: 11074.46 — Epsilon: 0.211\n",
            "Episode 312/500 — Total Reward: 15225.09 — Epsilon: 0.210\n",
            "Episode 313/500 — Total Reward: 1695.10 — Epsilon: 0.209\n",
            "Episode 314/500 — Total Reward: 1142.81 — Epsilon: 0.208\n",
            "Episode 315/500 — Total Reward: -1177.19 — Epsilon: 0.207\n",
            "Episode 316/500 — Total Reward: 9231.49 — Epsilon: 0.206\n",
            "Episode 317/500 — Total Reward: 5301.75 — Epsilon: 0.205\n",
            "Episode 318/500 — Total Reward: 3420.78 — Epsilon: 0.204\n",
            "Episode 319/500 — Total Reward: 14190.03 — Epsilon: 0.203\n",
            "Episode 320/500 — Total Reward: 2789.23 — Epsilon: 0.202\n",
            "Episode 321/500 — Total Reward: 959.86 — Epsilon: 0.201\n",
            "Episode 322/500 — Total Reward: 54894.34 — Epsilon: 0.200\n",
            "Episode 323/500 — Total Reward: 33722.53 — Epsilon: 0.199\n",
            "Episode 324/500 — Total Reward: 19873.16 — Epsilon: 0.198\n",
            "Episode 325/500 — Total Reward: 25000.61 — Epsilon: 0.197\n",
            "Episode 326/500 — Total Reward: 8333.98 — Epsilon: 0.196\n",
            "Episode 327/500 — Total Reward: 37387.52 — Epsilon: 0.195\n",
            "Episode 328/500 — Total Reward: 25568.68 — Epsilon: 0.194\n",
            "Episode 329/500 — Total Reward: 19963.20 — Epsilon: 0.193\n",
            "Episode 330/500 — Total Reward: 16585.46 — Epsilon: 0.192\n",
            "Episode 331/500 — Total Reward: -5441.05 — Epsilon: 0.191\n",
            "Episode 332/500 — Total Reward: 21765.61 — Epsilon: 0.190\n",
            "Episode 333/500 — Total Reward: 18746.09 — Epsilon: 0.189\n",
            "Episode 334/500 — Total Reward: 5440.75 — Epsilon: 0.188\n",
            "Episode 335/500 — Total Reward: 3752.02 — Epsilon: 0.187\n",
            "Episode 336/500 — Total Reward: 19296.62 — Epsilon: 0.187\n",
            "Episode 337/500 — Total Reward: 27437.55 — Epsilon: 0.186\n",
            "Episode 338/500 — Total Reward: 11442.47 — Epsilon: 0.185\n",
            "Episode 339/500 — Total Reward: 21156.53 — Epsilon: 0.184\n",
            "Episode 340/500 — Total Reward: 3634.91 — Epsilon: 0.183\n",
            "Episode 341/500 — Total Reward: 111.88 — Epsilon: 0.182\n",
            "Episode 342/500 — Total Reward: 6342.70 — Epsilon: 0.181\n",
            "Episode 343/500 — Total Reward: -6090.59 — Epsilon: 0.180\n",
            "Episode 344/500 — Total Reward: -928.60 — Epsilon: 0.179\n",
            "Episode 345/500 — Total Reward: 12408.59 — Epsilon: 0.178\n",
            "Episode 346/500 — Total Reward: -1754.81 — Epsilon: 0.177\n",
            "Episode 347/500 — Total Reward: 10845.65 — Epsilon: 0.177\n",
            "Episode 348/500 — Total Reward: 5695.45 — Epsilon: 0.176\n",
            "Episode 349/500 — Total Reward: 26223.11 — Epsilon: 0.175\n",
            "Episode 350/500 — Total Reward: 22065.13 — Epsilon: 0.174\n",
            "Episode 351/500 — Total Reward: 11679.56 — Epsilon: 0.173\n",
            "Episode 352/500 — Total Reward: 8128.77 — Epsilon: 0.172\n",
            "Episode 353/500 — Total Reward: 16991.85 — Epsilon: 0.171\n",
            "Episode 354/500 — Total Reward: 30798.20 — Epsilon: 0.170\n",
            "Episode 355/500 — Total Reward: 10098.99 — Epsilon: 0.170\n",
            "Episode 356/500 — Total Reward: 17501.83 — Epsilon: 0.169\n",
            "Episode 357/500 — Total Reward: 32267.42 — Epsilon: 0.168\n",
            "Episode 358/500 — Total Reward: 14637.00 — Epsilon: 0.167\n",
            "Episode 359/500 — Total Reward: 6895.99 — Epsilon: 0.166\n",
            "Episode 360/500 — Total Reward: 13659.07 — Epsilon: 0.165\n",
            "Episode 361/500 — Total Reward: 25703.43 — Epsilon: 0.165\n",
            "Episode 362/500 — Total Reward: 26434.98 — Epsilon: 0.164\n",
            "Episode 363/500 — Total Reward: 13007.38 — Epsilon: 0.163\n",
            "Episode 364/500 — Total Reward: 3605.26 — Epsilon: 0.162\n",
            "Episode 365/500 — Total Reward: 17581.94 — Epsilon: 0.161\n",
            "Episode 366/500 — Total Reward: 5489.85 — Epsilon: 0.160\n",
            "Episode 367/500 — Total Reward: 34996.31 — Epsilon: 0.160\n",
            "Episode 368/500 — Total Reward: 1891.10 — Epsilon: 0.159\n",
            "Episode 369/500 — Total Reward: 2856.61 — Epsilon: 0.158\n",
            "Episode 370/500 — Total Reward: 5378.82 — Epsilon: 0.157\n",
            "Episode 371/500 — Total Reward: 1411.65 — Epsilon: 0.157\n",
            "Episode 372/500 — Total Reward: 8168.43 — Epsilon: 0.156\n",
            "Episode 373/500 — Total Reward: 10520.72 — Epsilon: 0.155\n",
            "Episode 374/500 — Total Reward: 2808.88 — Epsilon: 0.154\n",
            "Episode 375/500 — Total Reward: 5847.71 — Epsilon: 0.153\n",
            "Episode 376/500 — Total Reward: 12239.93 — Epsilon: 0.153\n",
            "Episode 377/500 — Total Reward: 39663.73 — Epsilon: 0.152\n",
            "Episode 378/500 — Total Reward: 18260.09 — Epsilon: 0.151\n",
            "Episode 379/500 — Total Reward: 17873.77 — Epsilon: 0.150\n",
            "Episode 380/500 — Total Reward: 18899.09 — Epsilon: 0.150\n",
            "Episode 381/500 — Total Reward: 24225.04 — Epsilon: 0.149\n",
            "Episode 382/500 — Total Reward: 40674.75 — Epsilon: 0.148\n",
            "Episode 383/500 — Total Reward: -621.59 — Epsilon: 0.147\n",
            "Episode 384/500 — Total Reward: -613.12 — Epsilon: 0.147\n",
            "Episode 385/500 — Total Reward: 24100.36 — Epsilon: 0.146\n",
            "Episode 386/500 — Total Reward: 13553.64 — Epsilon: 0.145\n",
            "Episode 387/500 — Total Reward: 18931.75 — Epsilon: 0.144\n",
            "Episode 388/500 — Total Reward: 23113.35 — Epsilon: 0.144\n",
            "Episode 389/500 — Total Reward: -197.99 — Epsilon: 0.143\n",
            "Episode 390/500 — Total Reward: 50020.60 — Epsilon: 0.142\n",
            "Episode 391/500 — Total Reward: 21584.43 — Epsilon: 0.142\n",
            "Episode 392/500 — Total Reward: 9613.89 — Epsilon: 0.141\n",
            "Episode 393/500 — Total Reward: 33967.27 — Epsilon: 0.140\n",
            "Episode 394/500 — Total Reward: 20245.49 — Epsilon: 0.139\n",
            "Episode 395/500 — Total Reward: 11731.63 — Epsilon: 0.139\n",
            "Episode 396/500 — Total Reward: 2493.81 — Epsilon: 0.138\n",
            "Episode 397/500 — Total Reward: 24788.68 — Epsilon: 0.137\n",
            "Episode 398/500 — Total Reward: 6467.26 — Epsilon: 0.137\n",
            "Episode 399/500 — Total Reward: 38923.28 — Epsilon: 0.136\n",
            "Episode 400/500 — Total Reward: -646.89 — Epsilon: 0.135\n",
            "Episode 401/500 — Total Reward: 26334.69 — Epsilon: 0.135\n",
            "Episode 402/500 — Total Reward: 9320.75 — Epsilon: 0.134\n",
            "Episode 403/500 — Total Reward: -42.87 — Epsilon: 0.133\n",
            "Episode 404/500 — Total Reward: 48412.71 — Epsilon: 0.133\n",
            "Episode 405/500 — Total Reward: 30545.70 — Epsilon: 0.132\n",
            "Episode 406/500 — Total Reward: 1078.36 — Epsilon: 0.131\n",
            "Episode 407/500 — Total Reward: -588.47 — Epsilon: 0.131\n",
            "Episode 408/500 — Total Reward: 27914.17 — Epsilon: 0.130\n",
            "Episode 409/500 — Total Reward: 34137.16 — Epsilon: 0.129\n",
            "Episode 410/500 — Total Reward: -3041.36 — Epsilon: 0.129\n",
            "Episode 411/500 — Total Reward: 11038.41 — Epsilon: 0.128\n",
            "Episode 412/500 — Total Reward: 4599.63 — Epsilon: 0.127\n",
            "Episode 413/500 — Total Reward: 5694.15 — Epsilon: 0.127\n",
            "Episode 414/500 — Total Reward: 4433.04 — Epsilon: 0.126\n",
            "Episode 415/500 — Total Reward: -505.96 — Epsilon: 0.126\n",
            "Episode 416/500 — Total Reward: 12612.22 — Epsilon: 0.125\n",
            "Episode 417/500 — Total Reward: 21027.01 — Epsilon: 0.124\n",
            "Episode 418/500 — Total Reward: 26255.15 — Epsilon: 0.124\n",
            "Episode 419/500 — Total Reward: 21949.37 — Epsilon: 0.123\n",
            "Episode 420/500 — Total Reward: 575.97 — Epsilon: 0.122\n",
            "Episode 421/500 — Total Reward: 14322.31 — Epsilon: 0.122\n",
            "Episode 422/500 — Total Reward: 3718.02 — Epsilon: 0.121\n",
            "Episode 423/500 — Total Reward: 27056.11 — Epsilon: 0.121\n",
            "Episode 424/500 — Total Reward: 8042.18 — Epsilon: 0.120\n",
            "Episode 425/500 — Total Reward: 16582.15 — Epsilon: 0.119\n",
            "Episode 426/500 — Total Reward: 9360.73 — Epsilon: 0.119\n",
            "Episode 427/500 — Total Reward: 59609.09 — Epsilon: 0.118\n",
            "Episode 428/500 — Total Reward: 31829.05 — Epsilon: 0.118\n",
            "Episode 429/500 — Total Reward: 9364.63 — Epsilon: 0.117\n",
            "Episode 430/500 — Total Reward: 10525.61 — Epsilon: 0.116\n",
            "Episode 431/500 — Total Reward: 5015.64 — Epsilon: 0.116\n",
            "Episode 432/500 — Total Reward: 43486.80 — Epsilon: 0.115\n",
            "Episode 433/500 — Total Reward: -122.31 — Epsilon: 0.115\n",
            "Episode 434/500 — Total Reward: -1241.86 — Epsilon: 0.114\n",
            "Episode 435/500 — Total Reward: 28132.36 — Epsilon: 0.114\n",
            "Episode 436/500 — Total Reward: -1831.71 — Epsilon: 0.113\n",
            "Episode 437/500 — Total Reward: 15436.99 — Epsilon: 0.112\n",
            "Episode 438/500 — Total Reward: 7838.06 — Epsilon: 0.112\n",
            "Episode 439/500 — Total Reward: 8490.02 — Epsilon: 0.111\n",
            "Episode 440/500 — Total Reward: 36795.86 — Epsilon: 0.111\n",
            "Episode 441/500 — Total Reward: 30726.86 — Epsilon: 0.110\n",
            "Episode 442/500 — Total Reward: 16468.38 — Epsilon: 0.110\n",
            "Episode 443/500 — Total Reward: 2395.21 — Epsilon: 0.109\n",
            "Episode 444/500 — Total Reward: 92117.02 — Epsilon: 0.109\n",
            "Episode 445/500 — Total Reward: 91720.56 — Epsilon: 0.108\n",
            "Episode 446/500 — Total Reward: 90147.96 — Epsilon: 0.107\n",
            "Episode 447/500 — Total Reward: 102361.77 — Epsilon: 0.107\n",
            "Episode 448/500 — Total Reward: 99659.98 — Epsilon: 0.106\n",
            "Episode 449/500 — Total Reward: 117290.13 — Epsilon: 0.106\n",
            "Episode 450/500 — Total Reward: 9761.14 — Epsilon: 0.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path in your Drive\n",
        "model_path = \"/content/drive/MyDrive/trading_agent.pth\"\n",
        "\n",
        "# Save model state dict\n",
        "torch.save(policy_net.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC-n2OtKTWND",
        "outputId": "75d15e65-22ba-4cb0-cf9b-8400c1930106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/trading_agent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RQlHMrc5vmey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = QNetwork(input_dim=40, hidden_dim=128, output_dim=3).to(device)\n",
        "\n",
        "# 2. Load the saved weights\n",
        "#Corrected model path to the filename used during saving\n",
        "model_path = \"/content/drive/MyDrive/trading_agent.pth\"\n",
        "policy_net.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# 3. Set to evaluation mode\n",
        "policy_net.eval()\n",
        "\n",
        "print(\"✅ Model loaded and ready for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s30djlEfcS4O",
        "outputId": "cfdaeab2-638e-48ab-ec84-f40d051ff403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded and ready for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "95EXFliWWsJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(policy_net, test_states, test_prices):\n",
        "    env = TradingEnv(test_states, test_prices)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    portfolio_values = []\n",
        "\n",
        "    while not done:\n",
        "        # Always choose the best action (no ε-greedy during testing)\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = policy_net(state_tensor)\n",
        "        action_probs = action_probs = F.softmax(q_values, dim=1).detach().cpu().numpy().flatten()\n",
        "\n",
        "        #print(action_probs)\n",
        "        confidence=max(action_probs)\n",
        "        action = torch.argmax(q_values).item()\n",
        "        next_state, reward, done, balance, shares_held,price=env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        portfolio_values.append(env.total_asset)\n",
        "        # Explicitly convert price to a float using float()\n",
        "        print(f\"Action: {action} |Confidence: {confidence}| Balance: ${balance} | Price: ${price} | Shares Held: {shares_held} \")\n",
        "\n",
        "    # Explicitly convert price to a float using float()\n",
        "    print(f\"✅ Final Total Asset Value: ${float(shares_held)*float(price)+balance:.2f}\")\n",
        "\n",
        "    # Explicitly convert price to a float using float()\n",
        "    print(f\"📈 Total Test Reward: ${(float(shares_held)*float(price)+balance) -10000}\")\n",
        "\n",
        "    print(portfolio_values)"
      ],
      "metadata": {
        "id": "fUYuGA3afVOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prices = data['Close'].values[10:][test_mask]\n",
        "portfolio_history = test_agent(policy_net, test_states, test_prices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzrZnV06NQti",
        "outputId": "1caa4b9b-0073-4c89-c650-5f6a502221ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: 0 |Confidence: 1.0| Balance: $10000 | Price: $382.19 | Shares Held: 0 \n",
            "Action: 1 |Confidence: 1.0| Balance: $9617.86 | Price: $382.14 | Shares Held: 1 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9617.86 | Price: $373.11 | Shares Held: 1 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9617.86 | Price: $359.84 | Shares Held: 1 \n",
            "Action: 1 |Confidence: 1.0| Balance: $9260.0 | Price: $357.86 | Shares Held: 2 \n",
            "Action: 0 |Confidence: 0.9999544620513916| Balance: $9260.0 | Price: $354.56 | Shares Held: 2 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8869.51 | Price: $390.49 | Shares Held: 3 \n",
            "Action: 2 |Confidence: 1.0| Balance: $9250.86 | Price: $381.35 | Shares Held: 2 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8862.41 | Price: $388.45 | Shares Held: 3 \n",
            "Action: 0 |Confidence: 0.999832272529602| Balance: $8862.41 | Price: $387.81 | Shares Held: 3 \n",
            "Action: 2 |Confidence: 1.0| Balance: $9248.14 | Price: $385.73 | Shares Held: 2 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9248.14 | Price: $371.61 | Shares Held: 2 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9248.14 | Price: $367.78 | Shares Held: 2 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9248.14 | Price: $359.12 | Shares Held: 2 \n",
            "Action: 0 |Confidence: 1.0| Balance: $9248.14 | Price: $366.82 | Shares Held: 2 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8873.75 | Price: $374.39 | Shares Held: 3 \n",
            "Action: 0 |Confidence: 1.0| Balance: $8873.75 | Price: $387.3 | Shares Held: 3 \n",
            "Action: 0 |Confidence: 1.0| Balance: $8873.75 | Price: $391.85 | Shares Held: 3 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8482.59 | Price: $391.16 | Shares Held: 4 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8088.55 | Price: $394.04 | Shares Held: 5 \n",
            "Action: 2 |Confidence: 1.0| Balance: $8483.81 | Price: $395.26 | Shares Held: 4 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8058.41 | Price: $425.4 | Shares Held: 5 \n",
            "Action: 0 |Confidence: 1.0| Balance: $8058.41 | Price: $435.28 | Shares Held: 5 \n",
            "Action: 2 |Confidence: 1.0| Balance: $8494.58 | Price: $436.17 | Shares Held: 4 \n",
            "Action: 1 |Confidence: 1.0| Balance: $8061.2699999999995 | Price: $433.31 | Shares Held: 5 \n",
            "✅ Final Total Asset Value: $10227.82\n",
            "📈 Total Test Reward: $227.8199999999997\n",
            "[np.float64(10000.0), np.float64(9990.970000000001), np.float64(9977.7), np.float64(9975.720000000001), np.float64(9969.12), np.float64(10040.98), np.float64(10013.560000000001), np.float64(10027.76), np.float64(10025.84), np.float64(10019.6), np.float64(9991.359999999999), np.float64(9983.699999999999), np.float64(9966.38), np.float64(9981.779999999999), np.float64(9996.92), np.float64(10035.65), np.float64(10049.3), np.float64(10047.23), np.float64(10058.75), np.float64(10064.85), np.float64(10185.41), np.float64(10234.81), np.float64(10239.26), np.float64(10227.82), np.float64(10228.02)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_agent(test_states, test_prices, seed=42):\n",
        "    random.seed(seed)\n",
        "    env = TradingEnv(test_states, test_prices)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    portfolio_values = []\n",
        "\n",
        "    while not done:\n",
        "        action = random.choice([0, 1, 2])  # hold, buy, sell\n",
        "        next_state, reward, done,balance,shares_hold,price = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        portfolio_values.append(env.total_asset)\n",
        "\n",
        "    print(f\"🌀 Final Total Asset (Random): ${env.total_asset:.2f}\")\n",
        "    print(f\"🎲 Total Reward (Random): {total_reward:.2f}\")\n",
        "\n",
        "    return portfolio_values\n"
      ],
      "metadata": {
        "id": "WUa4DM-dRpPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_agent(test_states,test_prices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc_tJEoEehJp",
        "outputId": "dbf01f14-221c-44e3-ceef-c43ab513d929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌀 Final Total Asset (Random): $10081.60\n",
            "🎲 Total Reward (Random): 81.60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(10000.0),\n",
              " np.float64(10000.0),\n",
              " np.float64(10000.0),\n",
              " np.float64(10000.0),\n",
              " np.float64(9996.699999999999),\n",
              " np.float64(10032.63),\n",
              " np.float64(10023.49),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10030.59),\n",
              " np.float64(10043.5),\n",
              " np.float64(10048.050000000001),\n",
              " np.float64(10047.36),\n",
              " np.float64(10050.240000000002),\n",
              " np.float64(10051.460000000001),\n",
              " np.float64(10081.6),\n",
              " np.float64(10081.6),\n",
              " np.float64(10081.6),\n",
              " np.float64(10081.6),\n",
              " np.float64(10081.6)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}